{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data_loader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_loader.py\n",
    "\n",
    "\"\"\"\n",
    "data_loader\n",
    "~~~~~~~~~~~~\n",
    "Load the MNIST dataset.\n",
    "Use ``load()`` method to load dataset.\n",
    "Check the ``load()`` method for detail.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import urllib2\n",
    "import gzip\n",
    "import cPickle\n",
    "import numpy as np\n",
    "\n",
    "DATA_PATH = '../.tmp'\n",
    "DATA_FILENAME = 'mnist.pkl.gz'\n",
    "DATA_FILE = DATA_PATH + '/' + DATA_FILENAME\n",
    "\n",
    "def load():\n",
    "  '''\n",
    "  Load MNIST dataset.\n",
    "  This will return tuple of (training_data, validation_data, test_data)\n",
    "  ``training_data`` is a list of 50,000 training data. Each sample is a tuple ``(x, y)``.\n",
    "  ``x`` is a 784-dim np.ndarray contains the input image, and ``y`` is a 10-dim np.ndarray for one-hot label vector.\n",
    "  ``validataion_test`` and ``test_data`` are lists of 10,000 validataion and test data.\n",
    "  The structure is similar to ``training_data``, except ``y`` is a number corresponding to ``x`` input image.\n",
    "  '''\n",
    "  # download data if not exist\n",
    "  if not os.path.exists(DATA_FILE):\n",
    "    download()\n",
    "  \n",
    "  # load data\n",
    "  with gzip.open(DATA_FILE, 'rd') as file:\n",
    "    tr_dt, v_dt, t_dt = cPickle.load(file)\n",
    "  \n",
    "  # training data\n",
    "  inputs = [x.reshape((784, 1)) for x in tr_dt[0]]\n",
    "  labels = [label_2_vec(y) for y in tr_dt[1]]\n",
    "  training_data = zip(inputs, labels)\n",
    "  \n",
    "  # validation data\n",
    "  inputs = [x.reshape((784, 1)) for x in v_dt[0]]\n",
    "  validation_data = zip(inputs, v_dt[1])\n",
    "  \n",
    "  # test data\n",
    "  inputs = [x.reshape((784, 1)) for x in t_dt[0]]\n",
    "  test_data = zip(inputs, t_dt[1])\n",
    "  \n",
    "  return (training_data, validation_data, test_data)\n",
    "\n",
    "def download():\n",
    "  '''\n",
    "  Download MNIST dataset\n",
    "  '''\n",
    "  # create data dir if not exist\n",
    "  if not os.path.exists(DATA_PATH):\n",
    "    os.makedirs(DATA_PATH)\n",
    "  \n",
    "  # download\n",
    "  url = 'https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/mnist.pkl.gz'\n",
    "  input = urllib2.urlopen(url)\n",
    "  with open(DATA_FILE, 'wb') as output:\n",
    "    while True:\n",
    "      data = input.read(4096)\n",
    "      if data:\n",
    "        output.write(data)\n",
    "      else:\n",
    "        break\n",
    "    print('Downloaded MNIST dataset: '+ DATA_FILE)\n",
    "\n",
    "def label_2_vec(label):\n",
    "  '''\n",
    "  One-hot label vector\n",
    "  '''\n",
    "  v = np.zeros((10, 1))\n",
    "  v[label] = 1.0\n",
    "  return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile nn.py\n",
    "\"\"\"\n",
    "NN class\n",
    "~~~~~~~~~~~~~~~~\n",
    "Neural Network implement class.\n",
    "This NN use sigmoid as activation functions with cross-entropy cost function.\n",
    "\n",
    "Usage:\n",
    "1. Init NN\n",
    "nn = NN(layers)\n",
    "\n",
    "2. Train\n",
    "nn.train((x, y), epochs, mini_batch_size, eta)\n",
    "\n",
    "3. Predict\n",
    "y = nn.predict(x)\n",
    "\n",
    "4. Test / Evaluation\n",
    "correct_num = nn.evaluate(test_data)\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "class NN():\n",
    "    def __init__(self, layers):\n",
    "        \"\"\"\n",
    "        Init NN with ``layers`` size.\n",
    "        ``layers`` is array of layer sizes.\n",
    "        E.x: [3, 4, 5, 2] will create a NN of 4 layers.\n",
    "        In which,\n",
    "          input layer containts 3 nodes,\n",
    "         hidden layer 1 contains 4 nodes, \n",
    "         hidden layer 2 contains 5 nodes, \n",
    "         and, output layer contains 2 nodes\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.L = len(layers)\n",
    "        # ``w `` is a list (L-1) dim np.ndarray of matrix W for each layers\n",
    "        # ``w[0]` is layer 2 (hidden layer 1), ..., ``w[L-2]`` is output layer \n",
    "        # Each row hold weights for inputs (from before layer) of correspoding node (on current layer)\n",
    "        # The first column is bias for correspoding node\n",
    "        self.w = [np.random.randn(l2, l1 + 1) for l2, l1 in zip(layers[1:], layers[:-1])]\n",
    "        \n",
    "    def train(self, train_data, epochs, mini_batch_size, eta):\n",
    "        \"\"\"\n",
    "        Train NN with train data ``[(x, y)]``.\n",
    "        This use mini-batch SGD method to train the NN.\n",
    "        \"\"\"\n",
    "        # number of training data        \n",
    "        m = len(train_data)\n",
    "        # cost\n",
    "        cost = []\n",
    "        for j in range(epochs):\n",
    "            start_time = time.time()\n",
    "            print('Epoch {0} begin...'.format(j + 1))\n",
    "            # shuffle data before run\n",
    "            random.shuffle(train_data)\n",
    "            # divide data into mini batchs\n",
    "            for k in range(0, m, mini_batch_size):\n",
    "                mini_batch = train_data[k:k+mini_batch_size]\n",
    "                m_batch = len(mini_batch)\n",
    "                # calc gradient\n",
    "                w_grad = [np.zeros(W.shape) for W in self.w]\n",
    "                for x, y in mini_batch:\n",
    "                    grad = self.backprop(x, y)\n",
    "                    w_grad = [W_grad + g for W_grad, g in zip(w_grad, grad)]\n",
    "                w_grad = [W_grad / m_batch for W_grad in w_grad]\n",
    "                \n",
    "                # check grad for first mini_batch in first epoch\n",
    "                if j == 0  and k == 0 and not self.check_grad(mini_batch, w_grad):\n",
    "                    print('backprop fail!')\n",
    "                    return False\n",
    "                \n",
    "                # update w\n",
    "                self.w = [W - eta * W_grad for W, W_grad in zip(self.w, w_grad)]\n",
    "            \n",
    "            # calc cost\n",
    "            cost.append(self.cost(train_data))\n",
    "            print('Epoch {0} done: {1}'.format(j + 1, time.time() - start_time))\n",
    "            \n",
    "        return cost\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict label of single input data ``x``\n",
    "        \"\"\"\n",
    "        _, a = self.feedforward(x)\n",
    "        return np.argmax(a[-1])\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"\n",
    "        Evaluate NN with test data.\n",
    "        This will return the number of correct result\n",
    "        \"\"\"\n",
    "        results = [(self.predict(x), y) for (x, y) in test_data]\n",
    "        return sum(int(_y == y) for (_y, y) in results)\n",
    "    \n",
    "    def feedforward(self, x):\n",
    "        \"\"\"\n",
    "        Feedforward through network for calc ``z``,`` a``.\n",
    "        ``z`` is list of (L-1) vec-tor, ``z[0]`` for layer 2, and so on.\n",
    "        ``a`` is list of (L) vec-tor, ``a[0]`` for layer 1, and so on.\n",
    "        \"\"\"\n",
    "        z = []\n",
    "        a = [self.add_bias(x)]\n",
    "        for l in range(1, self.L):\n",
    "            z_l = np.dot(self.w[l-1], a[l-1])\n",
    "            a_l = self.sigmoid(z_l)\n",
    "            if l < self.L - 1:\n",
    "                a_l = self.add_bias(a_l)\n",
    "            \n",
    "            z.append(z_l)\n",
    "            a.append(a_l)\n",
    "            \n",
    "        return (z, a)\n",
    "    \n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"\n",
    "        Backpropagation to calc derivatives\n",
    "        \"\"\"\n",
    "        w_grad = [np.zeros(W.shape) for W in self.w]\n",
    "        # feedforward\n",
    "        z, a = self.feedforward(x)\n",
    "        # backward\n",
    "        dz = a[-1] - y\n",
    "        for _l in range(1, self.L):\n",
    "            l = -_l # layer index\n",
    "            if l < -1:\n",
    "                da = self.sigmoid_grad(z[l])\n",
    "                # do not calc for w_0 (da_0 / dz = 0 because of a_0 = 1 for all z)\n",
    "                dz = np.dot(self.w[l+1][:, 1:].transpose(), dz) * da\n",
    "            # gradient    \n",
    "            w_grad[l] = np.dot(dz, a[l-1].transpose())\n",
    "        \n",
    "        return w_grad\n",
    "    \n",
    "    def add_bias(self, a):\n",
    "        \"\"\"\n",
    "        add a_0 = 1 as input for bias w_0\n",
    "        \"\"\"\n",
    "        return np.insert(a, 0, 1, axis=0)\n",
    "    \n",
    "    def check_grad(self, data, grad, epsilon=1e-4, threshold=1e-6):\n",
    "        \"\"\"\n",
    "        Check gradient with:\n",
    "        * Epsilon      : 1e-4\n",
    "        * Threshold : 1e-6\n",
    "        \"\"\"\n",
    "        for l in range(self.L - 1):\n",
    "            n_row, n_col = self.w[l].shape\n",
    "            for i in range(n_row):\n",
    "                for j in range(n_col):\n",
    "                    w_l_ij = self.w[l][i][j]\n",
    "                    # left\n",
    "                    self.w[l][i][j] = w_l_ij - epsilon\n",
    "                    l_cost = self.cost(data)\n",
    "                    # right\n",
    "                    self.w[l][i][j] = w_l_ij + epsilon\n",
    "                    r_cost = self.cost(data)\n",
    "                    # numerical grad\n",
    "                    num_grad = (r_cost - l_cost) / (2 * epsilon)\n",
    "                    \n",
    "                    # diff\n",
    "                    diff = abs(grad[l][i][j] - num_grad)\n",
    "                    \n",
    "                    # reset w\n",
    "                    self.w[l][i][j] = w_l_ij\n",
    "                    \n",
    "                    if diff > threshold:\n",
    "                        print('Check Grad Error at (l: {0}, col: {1}, row: {2}), | num_grad: {3} vs backprop grad: {4} | : {5}'\n",
    "                              .format(l, i, j, num_grad, grad[l][i][j], diff))\n",
    "                        return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def cost(self, data):\n",
    "        \"\"\"\n",
    "        Return cross-entropy cost of NN on test data\n",
    "        \"\"\"\n",
    "        m = len(data)\n",
    "        j = 0\n",
    "        for x, y in data:\n",
    "            _, a = self.feedforward(x)\n",
    "            a_L = a[-1]\n",
    "            j += np.sum(np.nan_to_num(y*np.log(a_L) + (1-y)*np.log(1-a_L)))\n",
    "        return -j / m\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Sigmoid function use as activation function\n",
    "        \"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    def sigmoid_grad(self, z):\n",
    "        \"\"\"\n",
    "        Result derivative of sigmoid function\n",
    "        \"\"\"\n",
    "        s = self.sigmoid(z)\n",
    "        return s * (1 - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'urllib2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13536\\3798518277.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# load data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'training_data: {0} / validation_data: {1} / test_data: {2}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\Document6\\Exam\\AI\\Bai14-15NeuralNetwork\\data_loader.py\u001b[0m in \u001b[0;36mload\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m   \u001b[1;31m# download data if not exist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_FILE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m   \u001b[1;31m# load data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\Document6\\Exam\\AI\\Bai14-15NeuralNetwork\\data_loader.py\u001b[0m in \u001b[0;36mdownload\u001b[1;34m()\u001b[0m\n\u001b[0;32m     60\u001b[0m   \u001b[1;31m# download\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m   \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/mnist.pkl.gz'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m   \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_FILE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'urllib2' is not defined"
     ]
    }
   ],
   "source": [
    "import data_loader\n",
    "#from nn import NN\n",
    "\n",
    "# load data\n",
    "training_data, validation_data, test_data = data_loader.load()\n",
    "print('training_data: {0} / validation_data: {1} / test_data: {2}'.format(len(training_data), len(validation_data), len(test_data)))\n",
    "\n",
    "# run NN\n",
    "nn = NN([784, 100, 10])\n",
    "nn.train(training_data, 30, 100, 3.0)\n",
    "correct = nn.evaluate(test_data)\n",
    "total = len(test_data) \n",
    "print('Evaluation: {0} / {1} = {2}%'.format(correct, total, 100 * correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement cPickle (from versions: none)\n",
      "ERROR: No matching distribution found for cPickle\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "pip install cPickle"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "69aa3ff50fa730e7dd55677d5bf0877374a07602bea33ba8d28450eb030e234a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
